{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SeqNLP_Project1_Questions.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.14"
    },
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xT7MKZuMRaCg"
      },
      "source": [
        "# Sentiment Classification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Wq4RCyyPSYRp"
      },
      "source": [
        "## Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NGCtiXUhSWss",
        "colab": {}
      },
      "source": [
        "from keras.datasets import imdb\n",
        "\n",
        "vocab_size = 10000 #vocab size\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size) # vocab_size is no.of words to consider from the dataset, ordering based on frequency."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCPC_WN-eCyw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "vocab_size = 10000 #vocab size\n",
        "maxlen = 20  #number of word used from each review"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMEsHYrWxdtk",
        "colab_type": "text"
      },
      "source": [
        "## Train test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0g381XzeCyz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load dataset as a list of ints\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
        "#make all sequences of the same length\n",
        "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test =  pad_sequences(x_test, maxlen=maxlen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nt-MFvx1TFS6",
        "colab_type": "code",
        "outputId": "dfedaeb1-40ab-420c-f73d-5fa44c0209c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        }
      },
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25000, 20)\n",
            "(25000,)\n",
            "(25000, 20)\n",
            "(25000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDZtqoN-226P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_to_id = imdb.get_word_index()\n",
        "word_to_id = {k: (v+3) for k,v in word_to_id.items()}\n",
        "word_to_id[\"<PAD>\"] = 0\n",
        "word_to_id[\"<START>\"] = 1\n",
        "word_to_id[\"<UNK>\"] = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJca9Mzt88iM",
        "colab_type": "code",
        "outputId": "819e9335-024b-4dfd-c94e-b8091615be3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "id_to_word = {value:key for key, value in word_to_id.items()}\n",
        "print(' '.join(id_to_word[id] for id in x_train[6]))\n",
        "print('The sentiment is: ', y_train[6])"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "too much on <UNK> of head shots like most other films of the 80s and 90s do very good results\n",
            "('The sentiment is: ', 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dybtUgUReCy8",
        "colab_type": "text"
      },
      "source": [
        "## Build Keras Embedding Layer Model\n",
        "We can think of the Embedding layer as a dicionary that maps a index assigned to a word to a word vector. This layer is very flexible and can be used in a few ways:\n",
        "\n",
        "* The embedding layer can be used at the start of a larger deep learning model. \n",
        "* Also we could load pre-train word embeddings into the embedding layer when we create our model.\n",
        "* Use the embedding layer to train our own word2vec models.\n",
        "\n",
        "The keras embedding layer doesn't require us to onehot encode our words, instead we have to give each word a unqiue intger number as an id. For the imdb dataset we've loaded this has already been done, but if this wasn't the case we could use sklearn [LabelEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxNDNhrseCzA",
        "colab_type": "code",
        "outputId": "ebfdddac-1cda-48ab-f1a3-096a5088f990",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4J1cwBRtb2W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy import array\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "\n",
        "embeddings_dictionary = dict()\n",
        "word2vec_file = open('/content/drive/My Drive/NLP/Week 1 & 2/word2vec.glove.6B.50d.txt')\n",
        "\n",
        "for line in word2vec_file:\n",
        "    records = line.split()\n",
        "    word = records[0]\n",
        "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
        "    embeddings_dictionary [word] = vector_dimensions\n",
        "word2vec_file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVKZf_IUt-og",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "embedding_matrix = zeros((vocab_size, 20))\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_dictionary.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[index] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1TIzSnhuwrb",
        "colab_type": "text"
      },
      "source": [
        "Classification with Simple Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAndFByou1Ji",
        "colab_type": "code",
        "outputId": "fe64bef0-1fa4-402e-e1d0-87882a67eb5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        }
      },
      "source": [
        "import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Flatten\n",
        "model = Sequential()\n",
        "embedding_layer = Embedding(vocab_size, 20, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n",
        "model.add(embedding_layer)\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (None, 20, 20)            200000    \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 400)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 401       \n",
            "=================================================================\n",
            "Total params: 200,401\n",
            "Trainable params: 401\n",
            "Non-trainable params: 200,000\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwwhA5YgvTYU",
        "colab_type": "code",
        "outputId": "be6cdf53-b0f5-4315-d93a-8fafa43d2824",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        }
      },
      "source": [
        "model.fit(x_train, y_train, batch_size=128, epochs=6, verbose=1, validation_split=0.2)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/6\n",
            "20000/20000 [==============================] - 1s 56us/sample - loss: 0.6932 - acc: 0.4979 - val_loss: 0.6932 - val_acc: 0.4938\n",
            "Epoch 2/6\n",
            "20000/20000 [==============================] - 1s 35us/sample - loss: 0.6932 - acc: 0.5016 - val_loss: 0.6932 - val_acc: 0.4938\n",
            "Epoch 3/6\n",
            "20000/20000 [==============================] - 1s 35us/sample - loss: 0.6932 - acc: 0.4942 - val_loss: 0.6932 - val_acc: 0.4938\n",
            "Epoch 4/6\n",
            "20000/20000 [==============================] - 1s 36us/sample - loss: 0.6932 - acc: 0.5016 - val_loss: 0.6932 - val_acc: 0.4938\n",
            "Epoch 5/6\n",
            "20000/20000 [==============================] - 1s 37us/sample - loss: 0.6932 - acc: 0.5016 - val_loss: 0.6932 - val_acc: 0.4938\n",
            "Epoch 6/6\n",
            "20000/20000 [==============================] - 1s 35us/sample - loss: 0.6932 - acc: 0.5008 - val_loss: 0.6932 - val_acc: 0.4938\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1e4293f290>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwRiQDQ1vplp",
        "colab_type": "code",
        "outputId": "6b90e593-a9cb-4806-9c7b-9bce725d2f5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "score = model.evaluate(x_test, y_test, verbose=1)\n",
        "print(\"Test Score:\", score[0])\n",
        "print(\"Test Accuracy:\", score[1])"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000/25000 [==============================] - 2s 83us/sample - loss: 0.6931 - acc: 0.5000\n",
            "('Test Score:', 0.6931486254119873)\n",
            "('Test Accuracy:', 0.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrJtGSqhwC7s",
        "colab_type": "text"
      },
      "source": [
        "Classification with a Convolutional Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yxe2Pwd9wJmX",
        "colab_type": "code",
        "outputId": "f8cc1e43-4dda-4ded-fb25-2fa839094874",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D\n",
        "model1 = Sequential()\n",
        "\n",
        "embedding_layer = Embedding(vocab_size, 20, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n",
        "model1.add(embedding_layer)\n",
        "\n",
        "model1.add(Conv1D(128, 5, activation='relu'))\n",
        "model1.add(GlobalMaxPooling1D())\n",
        "model1.add(Dense(1, activation='sigmoid'))\n",
        "model1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "print(model1.summary())"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_5 (Embedding)      (None, 20, 20)            200000    \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 16, 128)           12928     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_1 (Glob (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 213,057\n",
            "Trainable params: 13,057\n",
            "Non-trainable params: 200,000\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzBdnIBsx8oT",
        "colab_type": "code",
        "outputId": "2c4118ad-8c4a-4500-c6f0-59db3939c775",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        }
      },
      "source": [
        "model1.fit(x_train, y_train, batch_size=32, epochs=6, verbose=1, validation_split=0.2)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/6\n",
            "20000/20000 [==============================] - 3s 160us/sample - loss: 0.6932 - acc: 0.5021 - val_loss: 0.6933 - val_acc: 0.4938\n",
            "Epoch 2/6\n",
            "20000/20000 [==============================] - 3s 133us/sample - loss: 0.6932 - acc: 0.4988 - val_loss: 0.6932 - val_acc: 0.4938\n",
            "Epoch 3/6\n",
            "20000/20000 [==============================] - 3s 126us/sample - loss: 0.6932 - acc: 0.4996 - val_loss: 0.6931 - val_acc: 0.5062\n",
            "Epoch 4/6\n",
            "20000/20000 [==============================] - 3s 127us/sample - loss: 0.6932 - acc: 0.4974 - val_loss: 0.6932 - val_acc: 0.4938\n",
            "Epoch 5/6\n",
            "20000/20000 [==============================] - 3s 127us/sample - loss: 0.6932 - acc: 0.4965 - val_loss: 0.6933 - val_acc: 0.4938\n",
            "Epoch 6/6\n",
            "20000/20000 [==============================] - 3s 130us/sample - loss: 0.6932 - acc: 0.4990 - val_loss: 0.6933 - val_acc: 0.4938\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1e41f64fd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpGThCT0xt_m",
        "colab_type": "code",
        "outputId": "926cb260-0350-49c6-f2fc-f8fbf1746834",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "score1 = model1.evaluate(x_test, y_test, verbose=1)\n",
        "print(\"Test Score:\", score1[0])\n",
        "print(\"Test Accuracy:\", score1[1])"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000/25000 [==============================] - 2s 87us/sample - loss: 0.6932 - acc: 0.5000\n",
            "('Test Score:', 0.693197723312378)\n",
            "('Test Accuracy:', 0.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QM0sDRZF0cWv",
        "colab_type": "text"
      },
      "source": [
        "Classification with Recurrent Neural Network (LSTM)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5OLM4eBeCy9",
        "colab_type": "code",
        "outputId": "d244858c-8e83-408a-eb8b-58dfd6666bb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        }
      },
      "source": [
        "model2 = Sequential()\n",
        "embedding_layer = Embedding(vocab_size, 20, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n",
        "model2.add(embedding_layer)\n",
        "model2.add(LSTM(128))\n",
        "\n",
        "model2.add(Dense(1, activation='sigmoid'))\n",
        "model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "print(model2.summary())"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_6 (Embedding)      (None, 20, 20)            200000    \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 128)               76288     \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 276,417\n",
            "Trainable params: 76,417\n",
            "Non-trainable params: 200,000\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvQY18s0p39i",
        "colab_type": "code",
        "outputId": "fab12033-41bc-4282-a424-ff9c251edf97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        }
      },
      "source": [
        "batch_size = 32\n",
        "model2.fit(x_train, y_train, epochs = 6, batch_size=batch_size, verbose = 2)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 25000 samples\n",
            "Epoch 1/6\n",
            "25000/25000 - 8s - loss: 0.6933 - acc: 0.5021\n",
            "Epoch 2/6\n",
            "25000/25000 - 6s - loss: 0.6932 - acc: 0.5036\n",
            "Epoch 3/6\n",
            "25000/25000 - 7s - loss: 0.6932 - acc: 0.4995\n",
            "Epoch 4/6\n",
            "25000/25000 - 7s - loss: 0.6932 - acc: 0.4990\n",
            "Epoch 5/6\n",
            "25000/25000 - 6s - loss: 0.6932 - acc: 0.4974\n",
            "Epoch 6/6\n",
            "25000/25000 - 6s - loss: 0.6932 - acc: 0.4966\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1e41ce05d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RatWU9ep937",
        "colab_type": "code",
        "outputId": "434b01ff-655d-4d58-d181-1b8b1e7a1241",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "score2 = model2.evaluate(x_test, y_test, verbose=1)\n",
        "print(\"Test Score:\", score2[0])\n",
        "print(\"Test Accuracy:\", score2[1])"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000/25000 [==============================] - 4s 145us/sample - loss: 0.6932 - acc: 0.5000\n",
            "('Test Score:', 0.6931525472640991)\n",
            "('Test Accuracy:', 0.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlmFcUyvqFZb",
        "colab_type": "text"
      },
      "source": [
        "We have seen very low accuracy scores for both training & test set with models based on Word2Vec models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sor5ESMlqPnz",
        "colab_type": "text"
      },
      "source": [
        "LSTM Models (Indepenedent of Word2Vec Model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsqKbsEzmkRx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_features = 20000\n",
        "model3 = Sequential()\n",
        "model3.add(Embedding(vocab_size, 128,input_length = x_train.shape[1]))\n",
        "model3.add(LSTM(128, dropout = 0.2, recurrent_dropout= 0.2))\n",
        "model3.add(Dense(1, activation= 'sigmoid'))\n",
        "model3.compile(loss = 'binary_crossentropy', optimizer= 'adam', metrics = ['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3CSVVPPeCzD",
        "colab_type": "code",
        "outputId": "6d533081-18c6-403a-a4b6-0da86e77c1ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        }
      },
      "source": [
        "batch_size = 32\n",
        "model3.fit(x_train, y_train, epochs = 6, batch_size=batch_size, verbose = 2)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 25000 samples\n",
            "Epoch 1/6\n",
            "25000/25000 - 50s - loss: 0.5274 - accuracy: 0.7306\n",
            "Epoch 2/6\n",
            "25000/25000 - 47s - loss: 0.4043 - accuracy: 0.8125\n",
            "Epoch 3/6\n",
            "25000/25000 - 46s - loss: 0.3259 - accuracy: 0.8522\n",
            "Epoch 4/6\n",
            "25000/25000 - 46s - loss: 0.2580 - accuracy: 0.8883\n",
            "Epoch 5/6\n",
            "25000/25000 - 46s - loss: 0.1976 - accuracy: 0.9182\n",
            "Epoch 6/6\n",
            "25000/25000 - 46s - loss: 0.1516 - accuracy: 0.9402\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1de0c544d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCJeBSwQYrpS",
        "colab_type": "code",
        "outputId": "42e979f2-3b76-4ff0-d8a8-4ba1cc925408",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "score3 = model3.evaluate(x_test, y_test, verbose=1)\n",
        "print(\"Test Score:\", score3[0])\n",
        "print(\"Test Accuracy:\", score3[1])"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000/25000 [==============================] - 11s 437us/sample - loss: 0.8656 - accuracy: 0.7468\n",
            "('Test Score:', 0.8656493499469757)\n",
            "('Test Accuracy:', 0.74676)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPtJ7Z2DxIFC",
        "colab_type": "text"
      },
      "source": [
        "We can see that both validation accuracy & test accuracy have improved and loss has gone down considerably when compared to previous models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Igq8Qm8GeCzG",
        "colab_type": "text"
      },
      "source": [
        "## Retrive the output of each layer in keras for a given single test sample from the trained model you built"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AqOnLa2eCzH",
        "colab_type": "code",
        "outputId": "7cf3b6db-233d-4d62-e1fb-87efa64de6de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "instance = ' '.join(id_to_word[id] for id in x_test[9000])\n",
        "print(instance)\n",
        "print('The sentiment is:', y_test[9000])"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "don't need to treat the audience as idiots overall there is just too much emotional melodrama in the whole movie\n",
            "('The sentiment is:', 0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkMQyWaGAgoa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "instance = tokenizer.texts_to_sequences(instance)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dUDSg7VeCzM",
        "colab_type": "code",
        "outputId": "cd5fb20f-29f2-4aa5-b8f1-ec7412723df0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        }
      },
      "source": [
        "flat_list = []\n",
        "for sublist in instance:\n",
        "    for item in sublist:\n",
        "        flat_list.append(item)\n",
        "\n",
        "flat_list = [flat_list]\n",
        "\n",
        "instance = pad_sequences(flat_list, maxlen=maxlen)\n",
        "\n",
        "print(model.predict(instance))\n",
        "print(model1.predict(instance))\n",
        "print(model2.predict(instance))\n",
        "print(model3.predict(instance))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.50085634]]\n",
            "[[0.5050242]]\n",
            "[[0.50163823]]\n",
            "[[0.00122061]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBN52SEnB8RY",
        "colab_type": "text"
      },
      "source": [
        "Positive outputs is mapped to 1 and negative outputs is mapped to 0. However, the sigmoid function floats output between 1 & 0. If the value is greater than 0.5, the sentiment is considered positive.The sentiment value for our single instance is greater than 0.5 for the 1st 3 models. That means the sentiment is positive. However, model 4 has predicted 0.0. This means that our sentiment is negative which actually is the case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sL5BttqtAes",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "outputId": "1fc81968-370d-49d9-9324-e2b51d0eddbe"
      },
      "source": [
        "from keras import backend as K\n",
        "inp = model3.input\n",
        "outputs = [layer.output for layer in model3.layers]\n",
        "functor = K.function([inp, K.learning_phase()], outputs)\n",
        "# Prediction_input_padded\n",
        "layer_outs = functor([instance, 0])\n",
        "print(layer_outs)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([[[ 0.0861014 , -0.06323227,  0.02154035, ..., -0.00451315,\n",
            "          0.04316535,  0.09930071],\n",
            "        [ 0.0861014 , -0.06323227,  0.02154035, ..., -0.00451315,\n",
            "          0.04316535,  0.09930071],\n",
            "        [ 0.0861014 , -0.06323227,  0.02154035, ..., -0.00451315,\n",
            "          0.04316535,  0.09930071],\n",
            "        ...,\n",
            "        [ 0.0861014 , -0.06323227,  0.02154035, ..., -0.00451315,\n",
            "          0.04316535,  0.09930071],\n",
            "        [ 0.0861014 , -0.06323227,  0.02154035, ..., -0.00451315,\n",
            "          0.04316535,  0.09930071],\n",
            "        [ 0.0861014 , -0.06323227,  0.02154035, ..., -0.00451315,\n",
            "          0.04316535,  0.09930071]]], dtype=float32), array([[ 0.644007  ,  0.11638042, -0.12205642, -0.034178  , -0.00887403,\n",
            "        -0.46166834,  0.00509549,  0.46002036,  0.15542595,  0.02199194,\n",
            "         0.32282138, -0.7592026 , -0.23874262,  0.09091439, -0.39348122,\n",
            "         0.08801773,  0.06043807, -0.33857736,  0.08257023,  0.25960174,\n",
            "         0.01355452, -0.60856134,  0.03751541,  0.26973736, -0.20418036,\n",
            "         0.08511097, -0.04335982,  0.28005272,  0.18595013, -0.04674038,\n",
            "        -0.50610614,  0.13052028,  0.17825215,  0.49944383, -0.01974079,\n",
            "        -0.07026688,  0.5114564 ,  0.16276073,  0.24554528,  0.19101368,\n",
            "         0.01874097, -0.59604347,  0.3173019 ,  0.17910422, -0.15439329,\n",
            "        -0.07809112, -0.7965598 , -0.37021497, -0.04530123, -0.5296726 ,\n",
            "        -0.04223303, -0.68315595, -0.30747488, -0.11975226, -0.42584142,\n",
            "        -0.44527602, -0.49420714,  0.18119831,  0.4169176 ,  0.07622916,\n",
            "         0.34613645,  0.01520309,  0.26422277,  0.4297139 , -0.11749312,\n",
            "        -0.07392084,  0.11048493, -0.5151116 , -0.02706273, -0.03140814,\n",
            "        -0.00618835, -0.00435881,  0.05156785,  0.42461735,  0.2517292 ,\n",
            "         0.30922973, -0.30583444,  0.06728362,  0.4011086 , -0.28869852,\n",
            "         0.03076039,  0.31581697,  0.02258624,  0.03831356, -0.33115548,\n",
            "        -0.10591982, -0.09492975,  0.34329772, -0.07414117, -0.02803745,\n",
            "         0.21150026, -0.06503539,  0.12932982,  0.5110941 ,  0.27313408,\n",
            "        -0.07287806,  0.34798834, -0.21297272,  0.06110158,  0.3141233 ,\n",
            "         0.66857153, -0.05321967,  0.07384052, -0.19442326,  0.22345367,\n",
            "         0.02624214, -0.03442298, -0.15312897,  0.01956463, -0.08589751,\n",
            "         0.04583676, -0.10082328,  0.15195511, -0.273514  ,  0.4728003 ,\n",
            "        -0.60166645,  0.05718933, -0.13872884,  0.112538  , -0.02555754,\n",
            "        -0.6164529 , -0.3551809 , -0.38201115,  0.02640365, -0.37351546,\n",
            "        -0.2713998 ,  0.02446766,  0.34366956]], dtype=float32), array([[0.00122061]], dtype=float32)]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}